{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cProfile\n",
    "import os\n",
    "from os import walk\n",
    "import matplotlib.pyplot as plt\n",
    "import pydicom as dicom\n",
    "import numpy as np\n",
    "from skimage import data, img_as_float\n",
    "from skimage import measure\n",
    "import math\n",
    "from tf_unet import unet, util, image_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation and Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image:\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        self.data_set = dicom.dcmread(name)\n",
    "        \n",
    "    def get_Image(self):\n",
    "        #returns pixel array of the image\n",
    "        return self.data_set.pixel_array\n",
    "    \n",
    "    def get_Kspace(self,flip):\n",
    "        #returns flipped k-space data\n",
    "        data = self.get_Image()\n",
    "        arr = np.fft.fft2(data)\n",
    "        return arr if not flip else np.fft.fftshift(arr)\n",
    "    \n",
    "    def skipEveryNLine(self,N,img):\n",
    "        data = img\n",
    "        step = N\n",
    "        i = 0\n",
    "        count = 0\n",
    "        while i < int(len(data)):\n",
    "            for j in range (0,int(len(data[i]))):\n",
    "                data[i][j]=0\n",
    "            count+=1\n",
    "            if(count>=N):\n",
    "                count = 0\n",
    "                i= (i+2) if i < int(len(data))else i\n",
    "            else:\n",
    "                i= (i+1) if i < int(len(data))else i\n",
    "        return data  \n",
    "\n",
    "    def grappaUndersample(self,accelaration_factor,centerline_factor):\n",
    "        #undersamples top and bottom imagex \n",
    "        flipped_Img = self.get_Kspace(True)\n",
    "        center_line = int(len(flipped_Img)/2)\n",
    "        centerline_factor = len(flipped_Img)/centerline_factor\n",
    "        \n",
    "        upper_border = int(center_line - ( len(flipped_Img)/centerline_factor)/2)\n",
    "        lower_border = int(center_line + ( len(flipped_Img)/centerline_factor)/2)\n",
    "        \n",
    "        upSubsample = None\n",
    "        downSubsample = None\n",
    "        \n",
    "        if(accelaration_factor == 0):\n",
    "            upSubsample = np.zeros((upper_border,320), dtype=int)\n",
    "            downSubsample = np.zeros((upper_border,320), dtype=int)\n",
    "        else:\n",
    "            upSubsample = self.skipEveryNLine(accelaration_factor,flipped_Img[0:upper_border])\n",
    "            downSubsample = self.skipEveryNLine(accelaration_factor,flipped_Img[lower_border+1:])\n",
    "         \n",
    "        return np.concatenate((upSubsample,flipped_Img[upper_border:lower_border+1] ,downSubsample))\n",
    "    \n",
    "    def showImage(self,img,inFourierDomain,title):\n",
    "        if not inFourierDomain:\n",
    "            plt.imshow(np.absolute(Img), cmap=plt.cm.bone), plt.title(title,plt.axis('off'))\n",
    "            \n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.axis('off')\n",
    "            plt.imshow(np.log(1+np.absolute(Img)), cmap=plt.cm.bone), plt.title(\"Original Image\")\n",
    "            plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourierTransform(img):\n",
    "    data = img\n",
    "    arr = np.fft.fft2(data)  \n",
    "    return arr\n",
    "\n",
    "def inverseFourierTransform(img):\n",
    "    data = img\n",
    "    fourierTransformImg = np.fft.ifft2(data)\n",
    "    return fourierTransformImg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import walk\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "def importDataset(path):\n",
    "    dcm_files = [path + '/' + f for f in listdir(path) if isfile(join(path, f))]\n",
    "    Images =  [Image(f) for f in dcm_files]\n",
    "    return Images\n",
    "def showImg(Img,inFourierDomain):\n",
    "        if not inFourierDomain:\n",
    "            plt.imshow(np.absolute(Img), cmap=plt.cm.bone), plt.title(\"Original Image\",plt.axis('off'))\n",
    "            \n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.axis('off')\n",
    "            plt.imshow(np.log(1+np.absolute(Img)), cmap=plt.cm.bone), plt.title(\"Original Image\")\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import dataset using the function below. Define path to the training dataset folder on \"mypath\" parameter and path to testing folder on \"test Path\" parameter. The function returns a set of Image objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320\n"
     ]
    }
   ],
   "source": [
    "mypath = './brain'\n",
    "# testPath = 'F:/fastMRI_brain_DICOM/183268758055'\n",
    "\n",
    "datasets = importDataset(mypath)\n",
    "# test_datasets = importDataset(testPath)\n",
    "dataset_shape = len(datasets[0].get_Image())\n",
    "dataset_column = len(datasets[0].get_Image()[0])\n",
    "print(dataset_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadImages(dataset,AF):\n",
    "    #edge cases \n",
    "    if not dataset:\n",
    "        raise Exception(\"Input a valid dataset\")\n",
    "    \n",
    "    if not AF: \n",
    "        raise Exception(\"Enter an accelaration factor\")\n",
    "        \n",
    "    if any ((x>4 or x<0) for x in AF):\n",
    "        raise Exception(\"The model only trains accelaration factors in the range of x where 0 <= x <= 4\")\n",
    "        \n",
    "    x_train = None\n",
    "    y_train = None\n",
    "       \n",
    "    for i in range(len(dataset)):\n",
    "        for j in range(len(AF)):\n",
    "            if not np.any(x_train) and not np.any(y_train) :\n",
    "                x_train = np.absolute(dataset[0].get_Image())\n",
    "                y_train = np.absolute(inverseFourierTransform(dataset[0].grappaUndersample(AF[0],20)))\n",
    "            \n",
    "            else:\n",
    "                x_train = np.vstack([x_train,np.absolute(dataset[i].get_Image())])\n",
    "                y_train = np.vstack([y_train,np.absolute(inverseFourierTransform(dataset[i].grappaUndersample(AF[j],20)))])\n",
    "    \n",
    "    x_train = x_train.reshape(len(dataset)*len(AF),320,320,1)\n",
    "    y_train = y_train.reshape(len(dataset)*len(AF),320,320,1)\n",
    "    return (x_train,y_train)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNet Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from keras.layers import Input, Dense, Activation, concatenate, UpSampling2D\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
    "from keras.losses import mean_squared_error, mean_absolute_error\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class uNet:\n",
    "    def __init__(self,activation):\n",
    "        self.hasArchitecture = False\n",
    "        self.model = None\n",
    "        self.activation = activation \n",
    "        self.history = None\n",
    "        self.kernels = (3,3)\n",
    "        self.strides = (1,1)\n",
    "    \n",
    "    def set_model(self,model):\n",
    "        self.model = model\n",
    "        self.hasArchitecture = True\n",
    "        self.history = model.history\n",
    "    \n",
    "    def train(self,subsampled,original,batch_size,num_ephocs,checkpoint_output):\n",
    "        if not self.hasArchitecture:\n",
    "            self.create_architecture()\n",
    "        \n",
    "        cp_path = checkpoint_output\n",
    "        model_path = os.path.join(cp_path,self.model.name)\n",
    "        #save model to another directory if directory already exists \n",
    "        i=1\n",
    "        while (os.path.exists(model_path)):\n",
    "            model_path=os.path.join(model_path,\"v_{}\".format(i))\n",
    "        os.mkdir(model_path)\n",
    "        cp_format = os.path.join(model_path, \"unet-{epoch:02d}.hdf5\")\n",
    "        \n",
    "        cp_callback = ModelCheckpoint(\n",
    "        #save checkpoints at every epoch\n",
    "        cp_format, save_freq='epoch')\n",
    "\n",
    "        print(\"Network checkpoints will be saved to: '{}'\".format(cp_path))\n",
    "\n",
    "        self.history = self.model.fit(\n",
    "        subsampled,\n",
    "        original,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_ephocs,\n",
    "        shuffle=True,\n",
    "        validation_split=.2,\n",
    "        callbacks=[cp_callback]\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.model.save(model_path) \n",
    "         \n",
    "    \n",
    "    \n",
    "    def create_architecture(self):\n",
    "        inputs = Input(shape=(dataset_shape,dataset_column,1))\n",
    "        \n",
    "        weights_init = RandomNormal(mean=0.0,stddev=0.1)\n",
    "        \n",
    "        #using padding = same is zero padding\n",
    "        zeroPadding = \"same\"\n",
    "        \n",
    "        layer1 = Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=self.kernels,\n",
    "            strides=self.strides,\n",
    "            padding=zeroPadding,\n",
    "            activation=self.activation,\n",
    "            kernel_initializer=weights_init)(inputs)\n",
    "        \n",
    "        layer2 = Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=self.kernels,\n",
    "            strides=self.strides,\n",
    "            padding=zeroPadding,\n",
    "            activation=self.activation,\n",
    "            kernel_initializer=weights_init)(layer1)\n",
    "        \n",
    "        maxpool1 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(layer2)\n",
    "        \n",
    "        layer3 = Conv2D(\n",
    "            filters=128,\n",
    "            kernel_size=self.kernels,\n",
    "            strides=self.strides,\n",
    "            padding=zeroPadding,\n",
    "            activation=self.activation,\n",
    "            kernel_initializer=weights_init)(maxpool1)\n",
    "        \n",
    "        layer4 = Conv2D(\n",
    "            filters=128,\n",
    "            kernel_size=self.kernels,\n",
    "            strides=self.strides,\n",
    "            padding=zeroPadding,\n",
    "            activation=self.activation,\n",
    "            kernel_initializer=weights_init)(layer3)\n",
    "        \n",
    "        maxpool2 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(layer4)\n",
    "        \n",
    "        layer5 = Conv2D(\n",
    "            filters=256,\n",
    "            kernel_size=self.kernels,\n",
    "            strides=self.strides,\n",
    "            padding=zeroPadding,\n",
    "            activation=self.activation,\n",
    "            kernel_initializer=weights_init)(maxpool2)\n",
    "        \n",
    "        layer6 = Conv2D(\n",
    "            filters=128,\n",
    "            kernel_size=self.kernels,\n",
    "            strides=self.strides,\n",
    "            padding=zeroPadding,\n",
    "            activation=self.activation,\n",
    "            kernel_initializer=weights_init)(layer5)\n",
    "        \n",
    "        averagepool1 =  concatenate([UpSampling2D(size=(2, 2))(layer6), layer4], axis=3)        \n",
    "        \n",
    "        layer7 = Conv2D(\n",
    "            filters=128,\n",
    "            kernel_size=self.kernels,\n",
    "            strides=self.strides,\n",
    "            padding=zeroPadding,\n",
    "            activation=self.activation,\n",
    "            kernel_initializer=weights_init)(averagepool1)\n",
    "        \n",
    "        layer8 = Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=self.kernels,\n",
    "            strides=self.strides,\n",
    "            padding=zeroPadding,\n",
    "            activation=self.activation,\n",
    "            kernel_initializer=weights_init)(layer7)\n",
    "        \n",
    "        averagepool2 = concatenate([UpSampling2D(size=(2, 2))(layer8), layer2], axis=3)\n",
    "        \n",
    "        layer9 = Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=self.kernels,\n",
    "            strides=self.strides,\n",
    "            padding=zeroPadding,\n",
    "            activation=self.activation,\n",
    "            kernel_initializer=weights_init)(averagepool2)\n",
    "        \n",
    "        layer10 = Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=self.kernels,\n",
    "            strides=self.strides,\n",
    "            padding=zeroPadding,\n",
    "            activation=self.activation,\n",
    "            kernel_initializer=weights_init)(layer9)\n",
    "        \n",
    "        finalOutput = Conv2D(\n",
    "            filters=1,\n",
    "            kernel_size=self.kernels,\n",
    "            strides=self.strides,\n",
    "            padding=zeroPadding,\n",
    "            activation=None,\n",
    "            kernel_initializer=weights_init)(layer10)\n",
    "        \n",
    "        self.model = Model(inputs=[inputs], outputs=[finalOutput])\n",
    "        \n",
    "        self.model.summary()\n",
    "        \n",
    "        self.model.compile(\n",
    "            optimizer=RMSprop( lr=.001, rho=0.9, epsilon=1e-08, decay=0),\n",
    "            loss=mean_squared_error,\n",
    "            metrics=[mean_squared_error])\n",
    "\n",
    "        self.hasArchitecture = True\n",
    "\n",
    "    def reconstruct(self,undersampled_img):\n",
    "        return self.model.predict(undersampled_img)\n",
    "    \n",
    "    def plot_model(self,metric):\n",
    "        plt.plot(self.history.history[metric])\n",
    "        plt.title(metric)\n",
    "        plt.ylabel(metric)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.yscale(\"log\")\n",
    "        plt.xscale( \"log\")\n",
    "        return self.history\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 320, 320, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 320, 320, 64) 640         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 320, 320, 64) 36928       conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 160, 160, 64) 0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 160, 160, 128 73856       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 160, 160, 128 147584      conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 80, 80, 128)  0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 80, 80, 256)  295168      max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 80, 80, 128)  295040      conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 160, 160, 128 0           conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 160, 160, 256 0           up_sampling2d_2[0][0]            \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 160, 160, 128 295040      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 160, 160, 64) 73792       conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 320, 320, 64) 0           conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 320, 320, 128 0           up_sampling2d_3[0][0]            \n",
      "                                                                 conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 320, 320, 64) 73792       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 320, 320, 64) 36928       conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 320, 320, 1)  577         conv2d_20[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,329,345\n",
      "Trainable params: 1,329,345\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Network checkpoints will be saved to: 'C:/MRI_dataset/output'\n",
      "Epoch 1/2000\n"
     ]
    }
   ],
   "source": [
    "#Specify prefered AFs\n",
    "AF=[1,2,3,4]\n",
    "original,subsampled = loadImages(datasets,AF)\n",
    "\n",
    "#Intialise model with prefered Activation Function\n",
    "net = uNet('relu')\n",
    "\n",
    "#Intialise batch size, number of epochs and output directory\n",
    "batch_size = 32\n",
    "epochs = 2000\n",
    "modelOutputDirectory = 'C:/MRI_dataset/output'\n",
    "\n",
    "#Train Model\n",
    "net.train(subsampled,original,batch_size,epochs,modelOutputDirectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following contains the code for reconstruction, error fixing and image/mse plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Eval():\n",
    "    def __init__(self,model):\n",
    "        m=uNet('relu')\n",
    "        m.set_model(model)\n",
    "        self.model = m\n",
    "        \n",
    "    def undersample_dataset(self,dataset,AF):\n",
    "        #edge cases \n",
    "        if not dataset:\n",
    "            raise Exception(\"Input a valid dataset\")\n",
    "\n",
    "        if not AF: \n",
    "            raise Exception(\"Enter an accelaration factor\")\n",
    "\n",
    "        if any ((x>4 or x<0) for x in AF):\n",
    "            raise Exception(\"The model only trains accelaration factors in the range of x where 0 <= x <= 4\")\n",
    "\n",
    "        undersampled_imgs ={}\n",
    "\n",
    "        for i in range(len(dataset)):\n",
    "            for j in range(len(AF)):\n",
    "                x = {\"{img}{AF}\".format(img=i , AF = AF[j]):np.absolute(inverseFourierTransform(dataset[i].grappaUndersample(AF[j],20)))\n",
    "                }\n",
    "                undersampled_imgs.update(x)\n",
    "\n",
    "        return undersampled_imgs\n",
    "    \n",
    "    def reconstruct(self,undersampled_img):\n",
    "        undersampled_input = undersampled_img.reshape(1,dataset_shape,dataset_shape,1)\n",
    "        recon_output = self.model.reconstruct(undersampled_input)\n",
    "        return np.squeeze(recon_output)\n",
    "    \n",
    "    def k_space_Correction(self,recon_img,original_img):\n",
    "        recon_fft = fourierTransform(recon_img)\n",
    "        original_fft = fourierTransform(original_img)\n",
    "\n",
    "        center_line = int(len(recon_fft)/2)\n",
    "\n",
    "        centerline_factor = 20\n",
    "\n",
    "        upper_border = int(center_line - ( centerline_factor)/2)\n",
    "        lower_border = int(center_line + ( centerline_factor)/2)\n",
    "\n",
    "        #Fix k-space of recon Image\n",
    "        recon_upper = recon_fft[0:upper_border]\n",
    "        recon_lower = recon_fft[lower_border+1:]\n",
    "        recon_centre = original_fft[upper_border:lower_border+1]\n",
    "        recon_fixed = np.concatenate((recon_upper,recon_centre ,recon_lower))\n",
    "\n",
    "        return np.absolute(inverseFourierTransform(recon_fixed))\n",
    "    \n",
    "    \n",
    "    #Plot MSE between recon Img and original Img\n",
    "    def plot_MSE(self,dataset,AF):\n",
    "        undersampled_imgs = self.undersample_dataset(dataset,AF)\n",
    "\n",
    "\n",
    "        k=0\n",
    "        for i,j in undersampled_imgs.items():\n",
    "\n",
    "            x = self.reconstruct(j)\n",
    "            y=dataset[k].get_Image()\n",
    "            \n",
    "            #Writes MSE to the console\n",
    "            mse = tf.keras.losses.MeanSquaredError()\n",
    "            print(\"Image {a} AF {b}\".format(a=k,b=i[len(i)-1]))\n",
    "            print(mse(y, x).numpy())\n",
    "\n",
    "\n",
    "            if(int(i[len(i)-1])==4):\n",
    "                k+=1\n",
    "        \n",
    "    #plot mse between recon img and k-space fixed recon img \n",
    "    def plot_kspace_fix_MSE(self,dataset,AF):\n",
    "        undersampled_imgs = self.undersample_dataset(dataset,AF)\n",
    "\n",
    "\n",
    "        k=0\n",
    "        for i,j in undersampled_imgs.items():\n",
    "\n",
    "            x = self.reconstruct(j)\n",
    "            y=dataset[k].get_Image()\n",
    "            x= self.k_space_Correction(x,y)\n",
    "\n",
    "            mse = tf.keras.losses.MeanSquaredError()\n",
    "            print(\"Image {a} AF {b}\".format(a=k,b=i[len(i)-1]))\n",
    "            print(mse(y, x).numpy())\n",
    "\n",
    "\n",
    "            if(int(i[len(i)-1])==4):\n",
    "                k+=1\n",
    "    \n",
    "    #plots undersampled images with reconstructed figures\n",
    "    def plot_imgs(self,dataset,AF,output_dir):\n",
    "        undersampled_imgs = self.undersample_dataset(dataset,AF)\n",
    "        directory = net.model.name\n",
    "\n",
    "        # Path \n",
    "        path = os.path.join(output_dir, directory)\n",
    "        if not os.path.exists(path):  \n",
    "            os.mkdir(path)\n",
    "        else:\n",
    "            z=1\n",
    "            while os.path.exists(path):\n",
    "                path = os.path.join(path,\"v{}\".format(z))\n",
    "\n",
    "        os.mkdir(path)\n",
    "\n",
    "        print(\"Images saved at '% s' \" % path)\n",
    "\n",
    "        k=0\n",
    "        for i,j in undersampled_imgs.items():\n",
    "\n",
    "            reconstructed_img = self.reconstruct(j)\n",
    "            print()\n",
    "\n",
    "            plt.figure(figsize=(15, 15))\n",
    "            plt.subplot(121), plt.imshow(j, cmap=plt.cm.bone) \n",
    "            plt.title(\"Undersampled Image {img} at AF={af}\".format(img=k,af=i[len(i)-1]),fontsize=25)\n",
    "            plt.axis('off')\n",
    "\n",
    "            plt.subplot(122), plt.imshow(reconstructed_img, cmap=plt.cm.bone)\n",
    "            plt.title('Reconstructed Image',fontsize=25)\n",
    "            plt.axis('off')\n",
    "\n",
    "            save_img = os.path.join(path, \"{}.png\".format(\"{}\".format(i)))\n",
    "            plt.savefig(save_img, bbox_inches='tight')\n",
    "            plt.close()\n",
    "\n",
    "            print(\"Saved {index} to {path}\".format(\n",
    "                index=i, path=save_img))\n",
    "            if(int(i[len(i)-1])==4):\n",
    "                k+=1\n",
    "                \n",
    "    #plot reconstructed images with k-space fixed recon img\n",
    "    def plot_k_spaceFix_imgs(dataset,AF,output_dir):\n",
    "        undersampled_imgs = self.undersample_dataset(dataset,AF)\n",
    "        directory = net.model.name\n",
    "\n",
    "        # Path \n",
    "        path = os.path.join(output_dir, directory)\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "            z=1\n",
    "            while os.path.exists(path):\n",
    "                path = os.path.join(path,\"v{}\".format(z))\n",
    "\n",
    "        os.mkdir(path)\n",
    "\n",
    "        print(\"Images saved at '% s' \" % path)\n",
    "\n",
    "        k=0\n",
    "        for i,j in undersampled_imgs.items():\n",
    "\n",
    "            reconstructed_img = self.reconstruct(j)\n",
    "            print()\n",
    "\n",
    "            plt.figure(figsize=(15, 15))\n",
    "            plt.subplot(121), plt.imshow(reconstructed_img, cmap=plt.cm.bone) \n",
    "            plt.title(\"Reconstructed Validation Image {img} at AF={af}\".format(img=k+14,af=i[len(i)-1]),fontsize=25)\n",
    "            plt.axis('off')\n",
    "\n",
    "            y=dataset[k].get_Image()\n",
    "            k_space_fix = self.k_space_Correction(reconstructed_img,y)\n",
    "\n",
    "            plt.subplot(122), plt.imshow(k_space_fix, cmap=plt.cm.bone)\n",
    "            plt.title('k-space fixed Image',fontsize=25)\n",
    "            plt.axis('off')\n",
    "\n",
    "            save_img = os.path.join(path, \"{}.png\".format(\"{}\".format(i)))\n",
    "            plt.savefig(save_img, bbox_inches='tight')\n",
    "            plt.close()\n",
    "\n",
    "            print(\"Saved {index} to {path}\".format(\n",
    "                index=i, path=save_img))\n",
    "            if(int(i[len(i)-1])==4):\n",
    "                k+=1\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('C:/MRI_dataset/output/functional_1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_imgs = Eval(net.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images saved at 'C:/test\\functional_1\\v1' \n",
      "\n",
      "Saved 01 to C:/test\\functional_1\\v1\\01.png\n",
      "\n",
      "Saved 02 to C:/test\\functional_1\\v1\\02.png\n",
      "\n",
      "Saved 03 to C:/test\\functional_1\\v1\\03.png\n",
      "\n",
      "Saved 11 to C:/test\\functional_1\\v1\\11.png\n",
      "\n",
      "Saved 12 to C:/test\\functional_1\\v1\\12.png\n",
      "\n",
      "Saved 13 to C:/test\\functional_1\\v1\\13.png\n",
      "\n",
      "Saved 21 to C:/test\\functional_1\\v1\\21.png\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-fa4a3e7f24a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0meval_imgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_imgs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"C:/test\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-37-763f155989d8>\u001b[0m in \u001b[0;36mplot_imgs\u001b[1;34m(self, dataset, AF, output_dir)\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mundersampled_imgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m             \u001b[0mreconstructed_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreconstruct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-37-763f155989d8>\u001b[0m in \u001b[0;36mreconstruct\u001b[1;34m(self, undersampled_img)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreconstruct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mundersampled_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mundersampled_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mundersampled_img\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataset_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataset_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mrecon_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreconstruct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mundersampled_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecon_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-cb3c1e0f1575>\u001b[0m in \u001b[0;36mreconstruct\u001b[1;34m(self, undersampled_img)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreconstruct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mundersampled_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mundersampled_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mplot_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[0;32m    129\u001b[0m           method.__name__))\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1597\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1598\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1599\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1600\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1601\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    812\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    815\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "eval_imgs.plot_imgs(datasets,[1,2,3],\"C:/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
